# generative video repository
A repo of generative video method paper

Year 2018

March
1. Probabilistic Video Generation using Holistic Attribute Control
  Â  Â https://arxiv.org/pdf/1803.08085.pdf

 Â  Â   Â Videos express highly structured spatio-temporal patterns of visual data. 
   Â  Â  two factors: 
 Â  Â  Â  Â    (i) temporally invariant (e.g., person identity), or slowly varying (e.g., activity), attribute- induced 
           appearance, encoding the persistent content of each frame, and 
 Â  Â  Â  Â  Â  (ii) an interframe motion or scene dynamics (e.g., encoding evolution of the person ex- ecuting the action). 
   Â  Â  VideoVAE
 Â  Â  Â   Â  -Â video generation + future prediction. 
 Â  Â  Â   Â  - generates a video (short clip) by:
 Â  Â  Â  Â  Â  Â   decoding samples sequentially drawn from a latent space distribution into full video frames.
 Â  Â  Â  Â  Â  Â   - VAE: encoding/decoding frames into/from the latent space 
 Â  Â  Â  Â  Â  Â   - RNN: model the dynamics in the latent space. Â  Â  
 Â  Â  Â  Â   - improve the video generation consistency through temporally-conditional sampling and quality
 Â  Â  Â  Â  Â  Â  Â structuring the latent space with attribute controls
 Â  Â  Â  Â  Â  Â  Â ensuring that attributes can be both inferred and conditioned on during learning/generation

2.Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks
   Â https://arxiv.org/pdf/1709.07592.pdf
 
 
 
3. Every Smile is Unique: Landmark-Guided Diverse Smile Generation
 Â    Â https://arxiv.org/pdf/1802.01873.pdf

Year 2017
 By the Way I like this stanford homework paper
 Â   http://cs231n.stanford.edu/reports/2017/pdfs/323.pdf
 

1. Dynamics Transfer GAN: Â (Dec 2017)
 Â  Â  Generating Video by Transferring Arbitrary Temporal Dynamics from a Source Video to a Single Target Image
 Â  Â Â https://arxiv.org/pdf/1712.03534.pdf

 Â  Â  Â   Â - spatial constructs <---- target image; dynamics <------source video sequence
 Â  Â  Â   Â - To preserve the spatial construct of the target image:
 Â  Â  Â  Â  Â  Â  Â - the appearance of the source video sequence is suppressed 
 Â  Â  Â  Â  Â  Â  Â - only the dynamics are obtained before being imposed onto the target image. 
 Â  Â  Â  Â  Â  Â  Â  Â  Â   Â (using the proposed appearance suppressed dynamics feature.)
 Â  Â  Â   Â - the spatial and temporal consistencies are verified via two discriminator networks. Â 
 Â  Â  Â  Â  Â  Â  Â  - discriminator A validates the fidelity of the generated frames appearance, 
 Â  Â  Â  Â  Â  Â  Â  -Â  B validates the dynamic consistency of the generated video sequence. 
               
 Â  Â  Â   Â - Results:
 Â  Â  Â  Â  Â  Â  Â  - successfully transferred arbitrary dynamics of the source video sequence onto a target image 
 Â  Â  Â  Â  Â  Â  Â  - maintained the spatial constructs (appearance) of the target image while generating spatially and   
 Â  Â  Â  Â  Â  Â  Â  Â   temporally consistent video sequences.
         [å›¾ç‰‡]
         
 Â  Â  Â  Â  Note: It is ### everything (Literature Review in its intro) because it is quite new.
 Â  Â @I didn't complete
 Â  Â 
2. Deep Video Generation, Prediction and Completion of Human Action Sequences
 Â   https://arxiv.org/pdf/1711.08682.pdf
 Â  Â  Â  Â  
3. Video Generation from Text 
 Â  Â https://arxiv.org/pdf/1710.00421.pdf
 Â   Hybrid VAE plus GAN
 Â   Two parts: Static( Using gist to sketch text-conditioned background color and object layout 
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (LSTM, RNN structure)
 Â  Â  Â  Â  Â  Â  Â  Dynamic ( A text2Filter. )
 Â  Â  Â  Â  Â  Â  Â  [å›¾ç‰‡]
               
 Â    Â 3 parts in Model (Section 3) 
 Â    Â  Â  Â - 3.1 Gist Generator
 Â    Â  Â  Â - 3.2 Video Generator
 Â  Â    Â  Â - 3.3 Text2Filter
  Â  Â  Note: Quite compact. Need time to digest
 Â  
4. Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks
  Â https://arxiv.org/pdf/1709.07592.pdf
 Â  
5. MoCoGAN: Decomposing Motion and Content for Video Generation
   Â https://arxiv.org/pdf/1707.04993.pdf
 Â  
6. To Create What You Tell: Generating Videos from Captions
 Â  Â  https://www.microsoft.com/en-us/research/wp-content/uploads/2017/11/BNI02-panA.pdf
 
 Â  Â  Â  Â Â -Temporal GANs conditioning on Captions, namely TGANs-C
 Â  Â  Â  Â  
 Â  Â  Â  Â  - transformed into a frame sequence with 3D spatio-temporal convolutions. 
 Â  Â  Â  Â  -  GAM evaluation metric ( Section 3.4 Experimental Setting)
 Â  Â  Â  Â  - Â Model Architecture 
 Â  Â  Â  Â  Â  Â  Â 3.1.1 Generator 
 Â  Â  Â  Â  Â  Â  Â  Â  Â   Given a sentence ð’®, a bi-LSTM is utilized to contextually embed the input word sequence, 
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â + a LSTM-based encoder to obtain the sentence representation S. 
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â + concatenated input of the sentence representation S and random noise variable z.
                     synthesize realistic videos with these
 Â  Â  Â  Â  Â  Â  Â 3.1.2 The discriminator network ð· includes three discriminators: 
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â a.video discriminator classifying realistic videos from generated+ optimizes video-caption matchingÂ  Â  Â  Â  Â  Â 
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â b. frame discriminator( between real and fake frames)and aligning frames with the conditioning caption
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â c. motion discriminator emphasizing that the adjacent frames in the generated videos run smoothly 
 Â  Â  Â  Â  Â  Â  Â 3.1.3 The whole part trained with 3 losses:
 Â  Â  Â  Â  Â  Â  Â  Â   Â  video-level matching-aware loss, frame-level matching-aware loss and temporal coherence loss 
                    
 
 Â  Â  Â  
 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â .
 Â  Â Year 2016
 Â  Â  Â  Â  Â  Â  Â  Â  Â  
 1. Generating Videos with Scene Dynamics
 Â  Â   https://arxiv.org/abs/1609.02612
 
 Â  Â  Â  Â  Â  - a spatio-temporal convolutional architecture 
 Â  Â  Â  Â  Â  - untangles the sceneâ€™s foreground from the background. 
 Â  Â  Â  Â  Â  - experiments show the model internally learns useful features for recognizing actions with minimal supervision,
 Â  Â  Â  Â  Â  - scene dynamics are a promising signal for representation learning.
 Â  Â  Â  Â  Â  [å›¾ç‰‡]
 Â  Â  Â  Â  Â  Slides : https://pdfs.semanticscholar.org/presentation/7188/6726f0a1b4075a7213499f8f25d7c9fb4143.pdf
           
 Â  Â  Â  Â  
